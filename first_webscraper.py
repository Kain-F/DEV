# -*- coding: utf-8 -*-
"""first webscraper

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DWfru7cI5nUxhQTdzcS6V-bYIesfyJkz

Colab notebooks allow you to combine **executable code** and **rich text** in a single document, along with **images**, **HTML**, **LaTeX** and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see [Overview of Colab](/notebooks/basic_features_overview.ipynb). To create a new Colab notebook you can use the File menu above, or use the following link: [create a new Colab notebook](http://colab.research.google.com#create=true).

Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see [jupyter.org](https://www.jupyter.org).

You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under [Working with Data](#working-with-data).
"""

from google.colab import drive
drive.mount('/content/drive')

! pip install bs4
! pip install lxml
import bs4

from urllib.request import urlopen
from bs4 import BeautifulSoup
from urllib.error import HTTPError
from urllib.error import URLError

html = urlopen('http://pythonscraping.com/pages/page1.html')
bs = BeautifulSoup(html.read(),'html.parser')
print(bs.h1)

bs = BeautifulSoup(html.read(),'html.parser')
print(bs)

bs = BeautifulSoup(html.read(),'lxml')

try:
  html = urlopen('https://pythonscrapingthisurldoesnotexist.com')
except HTTPError as e:
  print(e)
except URLError as e:
  print('The server could not be found!')
else:
  print('It worked')

html_2 = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
bs_2 = BeautifulSoup(html_2.read() ,'html.parser')

html_3 = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs_3 = BeautifulSoup(html_3,'html.parser')
images = bs_3.find_all('img', {'src':r'..\/img\/gifts/img.*.jpg'})
for image in images:
  print(image['src'])

namelist = bs_2.find_all('span',{'class':'green'})
for name in namelist:
  print(name.get_text())

i = 1
for i in range(1,100):
  link = 'https://app.colibris.be/abb/fiche.php?id=' + str(i)
  html_link = urlopen(link)
  bs_link = BeautifulSoup(html_link,'lxml')
  title = bs_link.findAll('h1')
  print(link,title)

i = 1
for i in range(1,100):
  link = 'https://app.colibris.be/abb/fiche.php?id=' + str(i)
  html_link = urlopen(link)
  bs_link = BeautifulSoup(html_link,'lxml')
  title = bs_link.findAllNext('h1')
  print(link,title)

html_wiki = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')
bs_wiki = BeautifulSoup(html_wiki,'html.parser')
for link in bs_wiki.findAll('a'):
  if 'href' in link.attrs:
    print(link.attrs['href'])

html_wiki = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')
bs_wiki = BeautifulSoup(html_wiki,'html.parser')

for link in bs_wiki.findAll('a'):
  if 'href' in link.attrs:
    print(link.attrs['href'])